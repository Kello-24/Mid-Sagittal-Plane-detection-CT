{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oropharynx patient data USZ preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requiremements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import csv\n",
    "import pandas as pd\n",
    "import shutil\n",
    "import numpy as np\n",
    "#from patient_id_coding.ipynb import process_patient_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = {\n",
    "\n",
    "    # Improved \"gtvp\" pattern\n",
    "    \"image\": r\"image\",\n",
    "\n",
    "    \"gtvp\": r\"\\b(?:klin|vorschlag|pr[ae]?eop|v1)?[\\s._-]*gtv[\\s._-]*p[\\s._-]*t?[\\s._-]*\\d*(?:new|rimary|pr[ae]?eop|v1|xxgy|74\\.4|70|ptv1)?[\\s._-]*(gy)?[\\s._-]*(1a)?[\\s._-]*(1b)?\\b\",\n",
    "\n",
    "    # Exact match for \"body\" with optional numbers or symbols following it\n",
    "    \"body\": r\"(?:^body[\\s._-]*\\d?$|^skin[\\s._-]*\\d?$)\",\n",
    "\n",
    "    # Improved \"spinal cord\" pattern for clearer boundary matching, including \"myelon\" as an alternative\n",
    "    \"spinal cord\": r\"(?:spinal[\\s._-]*cord$|^myelon$|myelon[\\s+]*5mm|spinal[\\s._-]*canal)\",\n",
    "\n",
    "    # Matches strings starting with \"mandib\"\n",
    "    \"mandibula\": r\"^mandib\",\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID encoding and decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_decode_patient_id(patient_id: str, coding='encode') -> str:\n",
    "    \"\"\"\n",
    "    Encodes or decodes a patient ID using a mapping from digits to letters.\n",
    "    \n",
    "    Parameters:\n",
    "    - patient_id (str): The patient ID as an 8-character string.\n",
    "        For encoding, the patient ID should consist of digits.\n",
    "        For decoding, the patient ID should consist of letters as defined in the mapping.\n",
    "    - coding (str): Operation mode: 'encode' maps digits to letters, \n",
    "                    'decode' converts letters back to digits.\n",
    "    \n",
    "    Returns:\n",
    "    - str: The processed patient ID (encoded or decoded).\n",
    "    \n",
    "    Raises:\n",
    "    - ValueError: If the patient_id does not meet the required format or if an invalid coding mode is provided.\n",
    "    \"\"\"\n",
    "    # Define a bijective mapping from digits to letters.\n",
    "    mapping = {\n",
    "        \"0\": \"Q\",\n",
    "        \"1\": \"W\",\n",
    "        \"2\": \"E\",\n",
    "        \"3\": \"R\",\n",
    "        \"4\": \"T\",\n",
    "        \"5\": \"Y\",\n",
    "        \"6\": \"U\",\n",
    "        \"7\": \"I\",\n",
    "        \"8\": \"O\",\n",
    "        \"9\": \"P\"\n",
    "    }\n",
    "    \n",
    "    # Create the inverse mapping: letters to digits.\n",
    "    reverse_mapping = {v: k for k, v in mapping.items()}\n",
    "    \n",
    "    def encode_patient_id(patient_id: str) -> str:\n",
    "        if len(patient_id) != 8 or not patient_id.isdigit():\n",
    "            raise ValueError(\"Patient ID for encoding must be an 8-digit string.\")\n",
    "        # Map each digit to its corresponding letter.\n",
    "        return ''.join(mapping[digit] for digit in patient_id)\n",
    "    \n",
    "    def decode_patient_id(encoded_id: str) -> str:\n",
    "        if len(encoded_id) != 8 or not all(char in reverse_mapping for char in encoded_id):\n",
    "            raise ValueError(\"Encoded ID must be an 8-character string with valid mapping letters.\")\n",
    "        # Map each letter back to the corresponding digit.\n",
    "        return ''.join(reverse_mapping[char] for char in encoded_id)\n",
    "    \n",
    "    if coding == 'encode':\n",
    "        return encode_patient_id(patient_id)\n",
    "    elif coding == 'decode':\n",
    "        return decode_patient_id(patient_id)\n",
    "    else:\n",
    "        raise ValueError(\"Coding must be either 'encode' or 'decode'.\")\n",
    "\n",
    "def process_patient_folders(root_dir: str, coding = 'encode'):\n",
    "    \"\"\"\n",
    "    Loops over all folders in the specified directory that match the pattern \"*[8 digits]\".\n",
    "    For each matching folder, the function encodes the patient ID (mapping digits to letters)\n",
    "    using the encode_decode_patient_id function (in 'encode' mode) and renames the folder.\n",
    "    \n",
    "    Parameters:\n",
    "    - root_dir (str): The root directory containing patient folders.\n",
    "    \"\"\"\n",
    "    # Regular expression pattern: an asterisk followed by exactly 8 digits.\n",
    "    pattern_encode = re.compile(r'(\\d{8})$')\n",
    "    pattern_decode = re.compile(r'([A-Z]{8})$')\n",
    "    \n",
    "    for folder in os.listdir(root_dir):\n",
    "        folder_path = os.path.join(root_dir, folder)\n",
    "        if os.path.isdir(folder_path):\n",
    "            if coding == 'encode':\n",
    "                pattern = pattern_encode\n",
    "            elif coding == 'decode':\n",
    "                pattern = pattern_decode\n",
    "\n",
    "            match = pattern.match(folder)\n",
    "\n",
    "            if match:\n",
    "                foldername = match.group(1)\n",
    "                # Encode the patient ID using our mapping (digits -> letters).\n",
    "                coded_id = encode_decode_patient_id(foldername, coding=coding)\n",
    "                new_folder_name = f\"{coded_id}\"\n",
    "                new_folder_path = os.path.join(root_dir, new_folder_name)\n",
    "                \n",
    "                # Check if a folder with the new name already exists.\n",
    "                if os.path.exists(new_folder_path):\n",
    "                    print(f\"Folder '{new_folder_name}' already exists. Skipping folder '{folder}'.\")\n",
    "                else:\n",
    "                    os.rename(folder_path, new_folder_path)\n",
    "                    print(f\"Renamed folder '{folder}' to '{new_folder_name}'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find patients with missing gtvp structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_missing_patterns(root_dir):\n",
    "    \"\"\"\n",
    "    Searches through folders in the specified root directory for .gz files whose names match given patterns.\n",
    "    \n",
    "    Folder selection:\n",
    "        - Only considers folders whose names are exactly 8 uppercase letters.\n",
    "        - Recursively inspects all subdirectories of these folders (e.g., renamed1, renamed2, etc.)\n",
    "    \n",
    "    Matching:\n",
    "        - For each .gz file, each filename is checked against a set of predefined regex patterns.\n",
    "        - For every pattern, only the first occurrence is taken as a valid match.\n",
    "    \n",
    "    CSV Output:\n",
    "        - Only folders missing a match for the \"gtvp\" pattern are written to the CSV.\n",
    "        - Additionally, for these folders, extra columns are added containing the matches from files\n",
    "          that match the additional simple pattern r\"gtv\" (one match per column).\n",
    "          \n",
    "    Parameters:\n",
    "        root_dir (str): The root directory in which to search for folders.\n",
    "    \n",
    "    Output:\n",
    "        A CSV file named \"missing_patterns.csv\" is created in the root directory.\n",
    "    \"\"\"\n",
    "    # Define the regex patterns for the original matching\n",
    "    patterns = {\n",
    "        \"image\": r\"image\",\n",
    "        \"gtvp\": r\"\\b(?:klin|vorschlag|pr[ae]?eop|v1)?[\\s._-]*gtv[\\s._-]*p[\\s._-]*t?[\\s._-]*\\d*(?:new|rimary|pr[ae]?eop|v1|xxgy|74\\.4|70|ptv1)?[\\s._-]*(gy)?[\\s._-]*(1a)?[\\s._-]*(1b)?\\b\",\n",
    "        \"body\": r\"(?:^body[\\s._-]*\\d?$|^skin[\\s._-]*\\d?$)\",\n",
    "        \"spinal cord\": r\"(?:spinal[\\s._-]*cord$|^myelon$|myelon[\\s+]*5mm|spinal[\\s._-]*canal)\",\n",
    "        \"mandibula\": r\"^mandib\"\n",
    "    }\n",
    "\n",
    "    # Compile regex patterns for case-insensitive matching\n",
    "    compiled_patterns = {key: re.compile(pattern, re.IGNORECASE) for key, pattern in patterns.items()}\n",
    "    \n",
    "    extra_regex = re.compile(r\".*gtv.*\", re.IGNORECASE)\n",
    "\n",
    "    \n",
    "    # List to store rows for CSV output (one row per folder with missing gtvp match)\n",
    "    missing_data = []\n",
    "    \n",
    "    # Iterate over entries in the root directory\n",
    "    for folder in os.listdir(root_dir):\n",
    "        folder_path = os.path.join(root_dir, folder)\n",
    "        # Only consider directories with exactly 8 uppercase letters\n",
    "        if os.path.isdir(folder_path) and re.fullmatch(r\"[A-Z]{8}\", folder):\n",
    "            # Dictionary to track whether a pattern was found in the current folder\n",
    "            found_patterns = {key: False for key in patterns.keys()}\n",
    "            # List to accumulate matches for the extra pattern \"gtv\"\n",
    "            gtv_matches = []\n",
    "            \n",
    "            # Walk recursively through the subdirectories of the folder\n",
    "            for subdir, dirs, files in os.walk(folder_path):\n",
    "                # Consider only files with the .gz extension\n",
    "                for file in files:\n",
    "                    file_lower = file.lower()\n",
    "                    if file_lower.endswith(\".gz\"):\n",
    "                        # If the file is a mask file, extract the substring between \"mask_\" and \".nii.gz\"\n",
    "                        if file_lower.startswith(\"mask_\") and file_lower.endswith(\".nii.gz\"):\n",
    "                            search_part = file_lower[len(\"mask_\"):-len(\".nii.gz\")]\n",
    "                        else:\n",
    "                            # For non-mask files (e.g., image files), use the full filename\n",
    "                            search_part = file_lower\n",
    "\n",
    "                        # Check for extra pattern \"gtv\" and accumulate the match if found\n",
    "                        if extra_regex.search(search_part):\n",
    "                            gtv_matches.append(search_part)\n",
    "                        \n",
    "                        # Check each original pattern (if not already found)\n",
    "                        for key, regex in compiled_patterns.items():\n",
    "                            if not found_patterns[key]:\n",
    "                                if regex.search(search_part):\n",
    "                                    found_patterns[key] = True\n",
    "            \n",
    "            # Only process folders missing a gtvp match\n",
    "            if not found_patterns[\"gtvp\"]:\n",
    "                # Create row data: folder name and subsequent columns for each gtv match\n",
    "                row = {\"folder\": encode_decode_patient_id(folder, coding='decode')}\n",
    "                for i, match in enumerate(gtv_matches):\n",
    "                    row[f\"match_{i+1}\"] = match\n",
    "                missing_data.append(row)\n",
    "    \n",
    "    # Determine the maximum number of \"gtv\" match columns among all rows\n",
    "    max_matches = 0\n",
    "    for row in missing_data:\n",
    "        count = sum(1 for key in row if key.startswith(\"match_\"))\n",
    "        if count > max_matches:\n",
    "            max_matches = count\n",
    "    \n",
    "    # Prepare the CSV header: one column for folder, then dynamic match columns\n",
    "    header = [\"folder\"] + [f\"match_{i+1}\" for i in range(max_matches)]\n",
    "    \n",
    "    # Write the results to a CSV file in the root directory\n",
    "    csv_file = os.path.join(root_dir, \"missing_patterns.csv\")\n",
    "    with open(csv_file, \"w\", newline=\"\") as f:\n",
    "        writer = csv.DictWriter(f, fieldnames=header)\n",
    "        writer.writeheader()\n",
    "        for row in missing_data:\n",
    "            # Ensure that each row has keys for all match columns\n",
    "            for i in range(max_matches):\n",
    "                key = f\"match_{i+1}\"\n",
    "                if key not in row:\n",
    "                    row[key] = \"\"\n",
    "            writer.writerow(row)\n",
    "    \n",
    "    print(f\"CSV file created at: {csv_file}\")\n",
    "\n",
    "# Example usage:\n",
    "root_path = r\"/home/loriskeller/Documents/Master Project/Patient data/patient_data_complete/06_midline_extraction\"\n",
    "#find_missing_patterns(root_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find all patient IDs from the second csv which occur in the first one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path1 = r\"/home/loriskeller/Documents/Master Project/Patient data/patient_data_complete/patients_no_adjuvant.csv\"\n",
    "csv_path2 = r\"/home/loriskeller/Documents/Master Project/Patient data/patient_data_complete/missing_patterns.csv\"\n",
    "\n",
    "\n",
    "def clean_patient_id(pid):\n",
    "    \"\"\"\n",
    "    Cleans a patient ID by converting it to a string, stripping leading/trailing whitespace,\n",
    "    and removing any surrounding single or double quotes.\n",
    "    The cleaned ID is then padded with zeros on the left to ensure it is 8 digits long.\n",
    "    \"\"\"\n",
    "    if pd.isna(pid):\n",
    "        return \"\"\n",
    "    cleaned = str(pid).strip().strip(\"'\").strip('\"')\n",
    "    return cleaned.zfill(8)\n",
    "\n",
    "def filter_csv2_with_common_ids(csv_file1, csv_file2, output_csv=\"filtered_csv2.csv\"):\n",
    "    \"\"\"\n",
    "    Loads two CSV files (with headers) and filters the second CSV (csv_file2) to keep only\n",
    "    rows whose patient IDs (from the first column) occur in the first CSV (csv_file1).\n",
    "\n",
    "    Patient IDs are cleaned before comparison to ensure proper matching even if they contain quotes,\n",
    "    and are zero-padded to be 8 digits long if necessary.\n",
    "\n",
    "    The filtered CSV is saved with patient IDs quoted to preserve leading zeros.\n",
    "    \n",
    "    Parameters:\n",
    "        csv_file1 (str): Path to the first CSV file.\n",
    "        csv_file2 (str): Path to the second CSV file.\n",
    "        output_csv (str): Path to save the filtered version of csv_file2.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The filtered version of csv_file2 with 8-digit patient IDs.\n",
    "    \"\"\"\n",
    "    # Load both CSV files with headers.\n",
    "    df1 = pd.read_csv(csv_file1)\n",
    "    df2 = pd.read_csv(csv_file2)\n",
    "    \n",
    "    # Clean the patient IDs from the first CSV.\n",
    "    ids1 = set(clean_patient_id(pid) for pid in df1.iloc[:, 0])\n",
    "    \n",
    "    # Create a temporary column in df2 with cleaned (and padded) patient IDs.\n",
    "    df2['cleaned_id'] = df2.iloc[:, 0].apply(clean_patient_id)\n",
    "    \n",
    "    # Filter df2: keep only rows where the cleaned patient ID occurs in ids1.\n",
    "    filtered_df2 = df2[df2['cleaned_id'].isin(ids1)].copy()\n",
    "    \n",
    "    # Update the original patient ID column in filtered_df2 with the cleaned version.\n",
    "    filtered_df2.iloc[:, 0] = filtered_df2['cleaned_id']\n",
    "    \n",
    "    # Drop the temporary 'cleaned_id' column.\n",
    "    filtered_df2.drop(columns=['cleaned_id'], inplace=True)\n",
    "    \n",
    "    # Save the filtered DataFrame to a new CSV file.\n",
    "    # The quoting parameter ensures that patient IDs remain as text with leading zeros.\n",
    "    filtered_df2.to_csv(output_csv, index=False, quoting=csv.QUOTE_ALL)\n",
    "    print(f\"Filtered CSV file created at: {output_csv}\")\n",
    "    \n",
    "    return filtered_df2\n",
    "\n",
    "def find_missing_ids(csv_file1, csv_file2, output_csv=\"missing_in_csv2.csv\"):\n",
    "    \"\"\"\n",
    "    Loads two CSV files (with headers) and finds the patient IDs from CSV1 (first column)\n",
    "    that are missing in CSV2. The patient IDs are cleaned and zero-padded to be 8 digits long.\n",
    "    \n",
    "    The function then filters CSV1 to keep only the rows with these missing patient IDs,\n",
    "    and saves the filtered DataFrame to a new CSV file with quoted fields.\n",
    "    \n",
    "    Parameters:\n",
    "        csv_file1 (str): Path to the first CSV file.\n",
    "        csv_file2 (str): Path to the second CSV file.\n",
    "        output_csv (str): Path to save the filtered version of CSV1 with missing patient IDs.\n",
    "    \n",
    "    Returns:\n",
    "        pandas.DataFrame: The filtered DataFrame containing rows from CSV1 whose patient IDs\n",
    "                          are missing in CSV2.\n",
    "    \"\"\"\n",
    "    # Load both CSV files (with headers)\n",
    "    df1 = pd.read_csv(csv_file1)\n",
    "    df2 = pd.read_csv(csv_file2)\n",
    "    \n",
    "    # Create a temporary column with cleaned (and padded) patient IDs for both dataframes.\n",
    "    df1['cleaned_id'] = df1.iloc[:, 0].apply(clean_patient_id)\n",
    "    df2['cleaned_id'] = df2.iloc[:, 0].apply(clean_patient_id)\n",
    "    \n",
    "    # Compute sets of cleaned patient IDs\n",
    "    ids1 = set(df1['cleaned_id'])\n",
    "    ids2 = set(df2['cleaned_id'])\n",
    "    \n",
    "    # Find patient IDs in CSV1 that are missing in CSV2\n",
    "    missing_ids = ids1 - ids2\n",
    "    \n",
    "    # Filter CSV1 to keep only rows where the cleaned ID is in the missing set\n",
    "    missing_df = df1[df1['cleaned_id'].isin(missing_ids)].copy()\n",
    "    \n",
    "    # Update the original patient ID column with the cleaned version and drop the temporary column\n",
    "    missing_df.iloc[:, 0] = missing_df['cleaned_id']\n",
    "    missing_df.drop(columns=['cleaned_id'], inplace=True)\n",
    "    \n",
    "    # Save the filtered DataFrame to a new CSV file.\n",
    "    # The quoting parameter ensures the IDs remain as text with leading zeros visible.\n",
    "    missing_df.to_csv(output_csv, index=False, quoting=csv.QUOTE_ALL)\n",
    "    print(f\"Missing patient IDs CSV created at: {output_csv}\")\n",
    "    \n",
    "    return missing_df\n",
    "\n",
    "# filtered_df = filter_csv2_with_common_ids(csv_path1, csv_path2, output_csv= r\"/home/loriskeller/Documents/Master Project/Patient data/patient_data_complete/missing_gtvp_no_adjuvant.csv\")\n",
    "\n",
    "# no_gtvp_adjuvant = find_missing_ids(csv_path2, csv_path1, output_csv= r\"/home/loriskeller/Documents/Master Project/Patient data/patient_data_complete/missing_gtvp_adjuvant.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy all relevant structures in a new directory with folders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def copy_patient_files_from_csv(csv_file, source_dir, dest_dir):\n",
    "    \"\"\"\n",
    "    Reads a CSV file with headers: Patient ID, Extention, Position, GTVp, Body, Mandible, Spinal Cord.\n",
    "    For each patient, finds the corresponding folder (named as the patient ID; padded to 8 digits)\n",
    "    in source_dir, and copies:\n",
    "      - the file named \"image.nii.gz\", and \n",
    "      - for each structure column (if non-empty) the file whose name is given in that cell\n",
    "    from within the patient folder (searched recursively) to a new folder in dest_dir named after the patient ID.\n",
    "    \n",
    "    Before processing each patient, the function checks if the GTVp entry is empty.\n",
    "    If it is, the patient is skipped.\n",
    "    \n",
    "    Parameters:\n",
    "      csv_file (str): Path to the CSV file.\n",
    "      source_dir (str): Directory containing patient folders (each named as an 8-digit patient ID).\n",
    "      dest_dir (str): Destination directory where new patient folders (with selected files) will be created.\n",
    "    \n",
    "    Behavior:\n",
    "      - For each patient, if the GTVp column is empty, that patient is skipped.\n",
    "      - Otherwise, a new folder (dest_dir/patient_id) is created.\n",
    "      - The function searches for \"image.nii.gz\" plus each non-empty structure file (GTVp, Body, Mandible, Spinal Cord)\n",
    "        in the patient folder (searched recursively).\n",
    "      - When a file is found, it is copied to the corresponding destination folder.\n",
    "      - If a structure entry is empty or the file isn’t found, that file is skipped.\n",
    "    \"\"\"\n",
    "    # Read CSV file (assumed to have headers)\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Define the structure columns that we need to consider\n",
    "    structure_columns = [\"GTVp\", \"Body\", \"Mandible\", \"Spinal Cord\"]\n",
    "    \n",
    "    # Process each row (patient) in the CSV.\n",
    "    for idx, row in df.iterrows():\n",
    "        # Skip if Patient ID is missing or not a number.\n",
    "        if pd.isna(row[\"Patient ID\"]):\n",
    "            print(f\"Patient ID missing for row {idx}, skipping.\")\n",
    "            continue\n",
    "        try:\n",
    "            patient_id_num = float(row[\"Patient ID\"])\n",
    "        except Exception as e:\n",
    "            print(f\"Patient ID not valid for row {idx}, skipping.\")\n",
    "            continue\n",
    "        \n",
    "        # Convert the patient ID to an integer and pad to 8 digits.\n",
    "        patient_id = str(int(patient_id_num)).zfill(8)\n",
    "        \n",
    "        # Check if the GTVp entry is empty. If so, skip this patient.\n",
    "        gtvp_entry = str(row[\"GTVp\"]).strip()\n",
    "        if not gtvp_entry or gtvp_entry.lower() in ['nan', '']:\n",
    "            print(f\"Patient {patient_id} has an empty GTVp entry. Skipping.\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Processing patient: {patient_id}\")\n",
    "        \n",
    "        # Locate the patient folder in the source directory.\n",
    "        patient_folder = os.path.join(source_dir, patient_id)\n",
    "        if not os.path.isdir(patient_folder):\n",
    "            print(f\"Patient folder {patient_folder} not found, skipping patient {patient_id}.\")\n",
    "            continue\n",
    "        \n",
    "        # Create a new destination folder for this patient.\n",
    "        dest_patient_folder = os.path.join(dest_dir, patient_id)\n",
    "        if os.path.exists(dest_patient_folder):\n",
    "            print(f\"Destination folder {dest_patient_folder} already exists, skipping.\")\n",
    "            continue\n",
    "        os.makedirs(dest_patient_folder, exist_ok=True)\n",
    "        \n",
    "        # Build the list of files to search for: always \"image.nii.gz\" plus each structure file (if provided).\n",
    "        files_to_copy = []\n",
    "        # Always include the image file.\n",
    "        files_to_copy.append(\"image.nii.gz\")\n",
    "        \n",
    "        # For each structure column, if the cell is non-empty, add that file name to the list.\n",
    "        for col in structure_columns:\n",
    "            file_name = str(row[col]).strip()\n",
    "            if file_name and file_name.lower() not in ['nan', '']:\n",
    "                files_to_copy.append(file_name)\n",
    "        \n",
    "        # For each target file, search recursively in the patient folder and copy the first found instance.\n",
    "        for target_file in files_to_copy:\n",
    "            found = False\n",
    "            for root, dirs, files in os.walk(patient_folder):\n",
    "                if target_file in files:\n",
    "                    source_file_path = os.path.join(root, target_file)\n",
    "                    dest_file_path = os.path.join(dest_patient_folder, target_file)\n",
    "                    shutil.copy2(source_file_path, dest_file_path)\n",
    "                    print(f\"Copied {source_file_path} to {dest_file_path}\")\n",
    "                    found = True\n",
    "                    break  # only copy the first found instance for this file\n",
    "            if not found:\n",
    "                print(f\"File {target_file} not found for patient {patient_id}.\")\n",
    "\n",
    "\n",
    "# csv_file = r\"/home/loriskeller/Documents/Master Project/Patient data/patient_data_complete/Patients_with_not_seperated_gtvp_and_ln/patients_gtvgesamt.csv\"\n",
    "# source_dir = r\"/home/loriskeller/Documents/Master Project/Patient data/patient_data_complete/06_midline_extraction\"\n",
    "# dest_dir = r\"/home/loriskeller/Documents/Master Project/Patient data/patient_data_complete/Patients_with_not_seperated_gtvp_and_ln\"\n",
    "# copy_patient_files_from_csv(csv_file, source_dir, dest_dir)\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a csv with structure names and extention info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_patient_structures(input_csv, patient_dir, output_csv, patterns):\n",
    "    \"\"\"\n",
    "    Processes patient folders based on an input CSV and searches for structure files.\n",
    "    \n",
    "    The input CSV must have headers: Patient ID, Location, Extention, Position.\n",
    "    The output CSV will have headers: Patient ID, Extention, Position, GTVp, Body, Mandible, Spinal Cord.\n",
    "    \n",
    "    For each patient (row in the CSV):\n",
    "      - The patient ID is padded with zeros to ensure it is 8 digits.\n",
    "      - The matching folder (named exactly as the padded Patient ID) in patient_dir is used.\n",
    "      - The function recursively searches all subfolders for .nii.gz files.\n",
    "      - For each file:\n",
    "           * If the file is a mask file (filename starts with \"mask_\" and ends with \".nii.gz\"),\n",
    "             the search text is the substring between \"mask_\" and \".nii.gz\".\n",
    "           * If the file is named exactly \"image.nii.gz\", the search text is \"image\".\n",
    "      - For each structure pattern (keys in the patterns dictionary, which must match the output CSV headers),\n",
    "        if the search text matches the pattern and that structure hasn’t been recorded yet, the file name is recorded.\n",
    "      - The search stops once all structure patterns have been found or after all files have been examined.\n",
    "      - If a structure is not found, its column remains empty.\n",
    "    \n",
    "    Parameters:\n",
    "      input_csv (str): Path to the input CSV with patient data.\n",
    "      patient_dir (str): Directory containing patient folders (named by Patient ID, 8 digits).\n",
    "      output_csv (str): Path to save the output CSV.\n",
    "      patterns (dict): Dictionary of regex patterns with keys matching structure names in the output CSV.\n",
    "    \n",
    "    Returns:\n",
    "      None. The output CSV is written to disk.\n",
    "    \"\"\"\n",
    "    # Read the input CSV (with headers: Patient ID, Location, Extention, Position)\n",
    "    df_in = pd.read_csv(input_csv)\n",
    "    \n",
    "    # Define output columns: from input (Patient ID, Extention, Position) + structure columns (from patterns)\n",
    "    structure_keys = list(patterns.keys())  # Expected order: e.g., [\"GTVp\", \"Body\", \"Mandible\", \"Spinal Cord\"]\n",
    "    output_columns = [\"Patient ID\", \"Extention\", \"Position\"] + structure_keys\n",
    "    \n",
    "    # Compile the regex patterns with case-insensitive matching.\n",
    "    compiled_patterns = {key: re.compile(pattern, re.IGNORECASE) for key, pattern in patterns.items()}\n",
    "    \n",
    "    # Prepare a list to store output rows.\n",
    "    output_rows = []\n",
    "    \n",
    "    # Process each patient from the input CSV.\n",
    "    for idx, row in df_in.iterrows():\n",
    "        # Pad the patient ID with zeros to ensure it is 8 digits.\n",
    "        patient_id = str(row[\"Patient ID\"]).strip().zfill(8)\n",
    "        extention = row[\"Extention\"]\n",
    "        position = row[\"Position\"]\n",
    "        \n",
    "        # Initialize output row with base columns.\n",
    "        output_row = {\n",
    "            \"Patient ID\": patient_id,\n",
    "            \"Extention\": extention,\n",
    "            \"Position\": position\n",
    "        }\n",
    "        # Initialize structure columns with empty strings.\n",
    "        for key in structure_keys:\n",
    "            output_row[key] = \"\"\n",
    "        \n",
    "        # Locate the patient folder (folder name is expected to be the 8-digit patient ID).\n",
    "        patient_folder = os.path.join(patient_dir, patient_id)\n",
    "        if not os.path.isdir(patient_folder):\n",
    "            print(f\"Patient folder '{patient_folder}' not found.\")\n",
    "            output_rows.append(output_row)\n",
    "            continue\n",
    "        \n",
    "        # Dictionary to track which structure patterns have been found.\n",
    "        found_patterns = {key: False for key in structure_keys}\n",
    "        \n",
    "        # Walk recursively through the patient folder.\n",
    "        for folderpath, subfolders, files in os.walk(patient_folder):\n",
    "            for file in files:\n",
    "                file_lower = file.lower()\n",
    "                if not file_lower.endswith(\".nii.gz\"):\n",
    "                    continue\n",
    "                \n",
    "                # Determine the search text.\n",
    "                if file_lower.startswith(\"mask_\") and file_lower.endswith(\".nii.gz\"):\n",
    "                    # Extract the substring between \"mask_\" and \".nii.gz\".\n",
    "                    search_text = file_lower[len(\"mask_\"):-len(\".nii.gz\")]\n",
    "                elif file_lower == \"image.nii.gz\":\n",
    "                    search_text = \"image\"\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                # Check each structure pattern that hasn't been found.\n",
    "                for key in structure_keys:\n",
    "                    if not found_patterns[key]:\n",
    "                        regex = compiled_patterns.get(key)\n",
    "                        if regex.search(search_text):\n",
    "                            found_patterns[key] = True\n",
    "                            output_row[key] = file  # Optionally, use os.path.join(folderpath, file) for full path.\n",
    "                # Stop searching if all structure patterns have been found.\n",
    "                if all(found_patterns.values()):\n",
    "                    break\n",
    "            if all(found_patterns.values()):\n",
    "                break\n",
    "        \n",
    "        output_rows.append(output_row)\n",
    "    \n",
    "    # Create output DataFrame and save to CSV.\n",
    "    df_out = pd.DataFrame(output_rows, columns=output_columns)\n",
    "    df_out.to_csv(output_csv, index=False)\n",
    "    print(f\"Output CSV saved to {output_csv}\")\n",
    "\n",
    "\n",
    "# Example patterns (ensure these match the structure header names)\n",
    "patterns = {\n",
    "    \"GTVp\": r\"\\b(?:klin|vorschlag|pr[ae]?eop|v1)?[\\s._-]*gtv[\\s._-]*p[\\s._-]*t?[\\s._-]*\\d*(?:new|rimary|pr[ae]?eop|v1|xxgy|74\\.4|70|ptv1)?[\\s._-]*(gy)?[\\s._-]*(1a)?[\\s._-]*(1b)?\\b\",\n",
    "    \"Body\": r\"(?:^body[\\s._-]*\\d?$|^skin[\\s._-]*\\d?$)\",\n",
    "    \"Mandible\": r\"^mandib\",\n",
    "    \"Spinal Cord\": r\"(?:spinal[\\s._-]*cord$|^myelon$|myelon[\\s+]*5mm|spinal[\\s._-]*canal)\"\n",
    "}\n",
    "\n",
    "patient_no_adjuvant = r\"/home/loriskeller/Documents/Master Project/Patient data/patient_data_complete/patients_no_adjuvant.csv\"\n",
    "patient_dir = r\"/home/loriskeller/Documents/Master Project/Patient data/patient_data_complete/06_midline_extraction\"\n",
    "output_csv = r\"/home/loriskeller/Documents/Master Project/Patient data/patient_data_complete/structures_and_extention_no_adjuvant.csv\"\n",
    "\n",
    "# process_patient_structures(patient_no_adjuvant, patient_dir, output_csv, patterns)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
